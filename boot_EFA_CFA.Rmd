---
title: "Bootstrap factor EFA and CFA"
author: "David Evans"
date: "`r Sys.Date()`"
output: html_document
---

## Install and load packages

```{r, eval=FALSE}

packages <- c(
  "boot",
  "DiagrammeR",
  "EFA.dimensions",
  "EFAtools",
  "GPArotation",
  "grid",
  "gridExtra",
  "lavaan",
  "lavaanPlot",
  "naniar",
  "psy",
  "psych",
  "qgraph",
  "readr",
  "semTools",
  "semPlot",
  "tidyverse",
  "VIM")

for(pkg in packages){
  if(!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}

knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Set file path
path <- "/Users/davidevans/Library/CloudStorage/OneDrive-Personal/My Projects/UBham/G-TSK development/Data/Reverse_scored/"

data_file <- "reversed_ADLInterferenceAndAt-GTSK_DATA_2023-10-13_1447.csv"

# Combine path and file name
full_path <- file.path(path, data_file)

# Import data from csv
data <- read_csv(full_path)

# Get questionnaire variables (retain all variables except 'record_id')
questionnaire_vars <- names(data)[!(names(data) %in% "record_id")]

# Count number of questionnaire variables
num_vars <- length(questionnaire_vars)
print(paste("There are", num_vars, "variables (items) in this dataset"))

knitr::opts_chunk$set(echo = TRUE)
```


```{r}

# Set seed
seed <- 1234
set.seed(seed)

# Set number of bootstrap iterations
boot_iterations <- 1000 # Set to 1000 or more

# Minimum number of subjects per item for EFA
min_subjects_item <- 20

knitr::opts_chunk$set(echo = TRUE)
```

## Assess missing data

```{r}

# Function to calculate missing values
calc_missing_values <- function(dataset, vars) {
  sum(is.na(dataset[, vars]))
}

# Initialize an empty character vector to hold all the statements
miss_data_statements <- c()

# Initial count of rows
initial_row_count <- nrow(data)
miss_data_statements <- c(miss_data_statements, sprintf("Total number of rows at beginning: %d", initial_row_count))

# Initial count of missing values
total_missing_values <- calc_missing_values(data, questionnaire_vars)
miss_data_statements <- c(miss_data_statements, sprintf("Total missing values before cleaning: %d", total_missing_values))

# Count rows where all questionnaire items are NA
all_missing_rows <- sum(rowSums(is.na(data[, questionnaire_vars])) == length(questionnaire_vars))
miss_data_statements <- c(miss_data_statements, sprintf("Number of rows with all values missing: %d", all_missing_rows))

# Remove rows in which all values are NA
data <- data %>%
  filter(rowSums(is.na(.[, questionnaire_vars])) < length(questionnaire_vars))

# Count the number of remaining rows after removing those with all items missing
remaining_rows_after_all_na_removal <- nrow(data)
miss_data_statements <- c(miss_data_statements, sprintf("Number of rows after removing those with all values missing: %d", remaining_rows_after_all_na_removal))

# Perform Little's Missing Completely At Random test
mcar_result <- naniar::mcar_test(data[, questionnaire_vars])
mcar_statistic <- mcar_result$statistic
mcar_df <- mcar_result$df
mcar_p_value <- mcar_result$p.value
missing_patterns <- mcar_result$missing.patterns
mcar_statement <- sprintf("Little's MCAR test yielded a chi-squared statistic of %.2f with %d degrees of freedom, resulting in a p-value of %.3f. The data contains %d unique missing data patterns.", 
                            mcar_statistic, mcar_df, mcar_p_value, missing_patterns)
miss_data_statements <- c(miss_data_statements, mcar_statement)

# Combine all statements into a single string, separated by newlines
miss_data_output <- paste(miss_data_statements, collapse = "\n")

# Print and save the final output to a text file
print(miss_data_output)
writeLines(miss_data_output, file.path(path, "Missing_data.txt"))

knitr::opts_chunk$set(echo = TRUE)
```

### Visualise missing values

```{r}

# Function to visualize and save item response distributions
plot_response_distribution <- function(data, columns, plot_title, save_path) {
  
  # Convert data to long format and convert NA to "Missing"
  long_data <- data %>%
    tidyr::pivot_longer(cols = columns, names_to = "item", values_to = "response") %>%
    mutate(
      response = ifelse(is.na(response), "Missing", as.character(response)),
      item = factor(item, levels = columns)  # Set the levels of item factor to the order of columns
    )
  
  # Calculate the total number of non-missing responses for each item
  total_non_missing <- long_data %>%
    group_by(item) %>%
    summarise(total = sum(response != "Missing", na.rm = TRUE))
  
  # Calculate counts
  long_data <- long_data %>%
    group_by(item, response) %>%
    summarise(count = n()) %>%
    left_join(total_non_missing, by = "item") %>%
    mutate(proportion = count / total)
  
  # Compute a suitable y-axis limit
  y_max <- max(long_data$count) * 1.1
  
  # Define colors for each response category
  response_colors <- scales::brewer_pal(palette = "Set2")(length(unique(long_data$response)))
  names(response_colors) <- unique(long_data$response)
  
  # Create bar plots
  p <- ggplot(long_data, aes(x = response, y = count, fill = response)) +
    geom_bar(stat = "identity") +
    scale_fill_manual(values = response_colors, name = "Response") +
    geom_text(aes(label = count), position = position_stack(vjust = 0.5), color = "black") +
    geom_text(data = subset(long_data, response != "Missing"), aes(label = scales::percent(proportion, accuracy = 0.1), y = count), vjust = -0.5, color = "black") +
    facet_wrap(~item, scales = "free_x") +
    labs(title = plot_title) +
    ylab("Count") +
    ylim(0, y_max) +
    theme_minimal()
  
  # Print and save the plot
  print(p)
  full_save_path <- file.path(path, save_path)
  ggsave(full_save_path, plot = p, width = 10, height = 8)
}

# Plot response distribution
plot_response_distribution(data, questionnaire_vars, "Response Distribution Before Imputation", "response_distribution_before_imputation.png")

```

### Impute missing values

```{r}

# Compute the number of values that are to be imputed after removing rows with all missing values
values_to_be_imputed <- calc_missing_values(data, questionnaire_vars)
cat("Number of values to be imputed:", values_to_be_imputed, "\n")

# Count rows with at least one NA value
any_missing_rows <- sum(rowSums(is.na(data[, questionnaire_vars])) > 0)
cat("Number of rows with one or more values missing:", any_missing_rows, "\n")

# Perform k-NN imputation on remaining data
knn_imputed_data <- VIM::kNN(data[, questionnaire_vars]) # Default k=5, but adjust as needed

# For each variable, replace NAs with imputed values
for (var in questionnaire_vars) {
  imp_col_name <- paste0(var, "_imp")
  # Identify indices of values that were imputed
  imputed_indices <- which(knn_imputed_data[[imp_col_name]] == TRUE)
  # Replace only values that were imputed
  data[imputed_indices, var] <- knn_imputed_data[imputed_indices, var]
}

# Descriptive statistics after imputation
cleaned_missing_values <- calc_missing_values(data, questionnaire_vars)
cat("Number of missing values after cleaning:", cleaned_missing_values, "\n") # Should be zero
cat("Descriptive statistics of cleaned data:\n")
print(head(summary(data[, questionnaire_vars])))

knitr::opts_chunk$set(echo = TRUE)
```

## Check internal consistency

```{r}

# Function to format output of (Chronbach's) alpha object
format_alpha_output <- function(alpha_obj, item_names) {
  output <- c()
  output <- c(output, sprintf("Raw alpha: %.3f", alpha_obj$total$raw_alpha))
  output <- c(output, "") # Add line spacing
  
  # Append item-total correlations with item names
  output <- c(output, "Item-total correlations:")
  for (i in 1:length(item_names)) {
    output <- c(output, sprintf("%s: %.3f", item_names[i], alpha_obj$item.stats$raw.r[i]))
  }
  output <- c(output, "") # Add line spacing
  
  # Append alpha if item dropped with item names
  output <- c(output, "Alpha if item dropped:")
  for (i in 1:length(item_names)) {
    output <- c(output, sprintf("%s: %.3f", item_names[i], alpha_obj$alpha.drop$raw_alpha[i]))
  }
  return(output)
}

# Check internal consistency within whole dataset
alpha <- psych::alpha(data[, questionnaire_vars])

# Format alpha output using the questionnaire_vars as item names
formatted_alpha_output <- format_alpha_output(alpha, questionnaire_vars)

# Add a title to the beginning of the formatted output
alpha_output_with_title <- c("Chronbach's alpha for whole dataset:", "", formatted_alpha_output)

# Save the formatted output with title to a text file
writeLines(alpha_output_with_title, file.path(path, "Chronbachs_Alpha_Whole_Dataset.txt"))

# Capture output of the Chronbach's alpha and add title
alpha_output <- capture.output(print(alpha))
alpha_output_with_title <- c("Chronbach's alpha for whole dataset:", "", alpha_output)

# Print and save captured output to a text file
print(alpha_output_with_title)
writeLines(alpha_output_with_title, file.path(path, "Chronbachs_Alpha_other.txt"))

```


### Prepare for EFA

```{r}

# Calculate the minimum number of subjects required to run EFA
min_subjects <- num_vars * min_subjects_item
cat("The minimum number of subjects required for EFA is:", min_subjects, "\n")

# Count the number of subjects (rows) available
num_subjects <- nrow(data)
cat("The available number of subjects is:", num_subjects, "\n")

# Calculate the split ratio if/when splitting for EFA:CFA (if not splitting, set to 1.0)
split_ratio <- min_subjects / num_subjects
print(paste("The minimum split ratio required would be", split_ratio))

# Define split ratio thresholds
lower_threshold <- 0.5
upper_threshold <- 0.8

# Adjust split ratio based on thresholds
if (split_ratio < lower_threshold) {
  print(paste("The split ratio has been increased from", split_ratio, "to", lower_threshold))
  split_ratio <- lower_threshold
} else if (split_ratio > upper_threshold) {  
  print(paste("Too few subjects available for the number of items to split the data."))
  split_ratio <- 1.0
}
print(paste("The split ratio has been set to", split_ratio))

# Use split ratio to split the data into two subsets: one for EFA and one for CFA
data_ids <- data$record_id
split_size <- ceiling(length(data_ids) * split_ratio) # round up to an integer
split_ids <- sample(data_ids, size = split_size)
efa_data <- data[data$record_id %in% split_ids, ]
cfa_data <- data[!(data$record_id %in% split_ids), ]

# Print sizes of subsets
print(paste("Number of subjects in EFA data:", nrow(efa_data)))
print(paste("Number of subjects in CFA data:", nrow(cfa_data)))

# Check internal consistency within each subset (if applicable)
if(split_ratio < 1.0) {
  efa_data_alpha <- psych::alpha(efa_data[, questionnaire_vars])
  print("Chronbach's alpha for EFA dataset:")
  print(efa_data_alpha)

  cfa_data_alpha <- psych::alpha(cfa_data[, questionnaire_vars])
  print("Chronbach's alpha for CFA dataset:")
  print(cfa_data_alpha)
}

knitr::opts_chunk$set(echo = TRUE)
```


```{r}

# Function to get number of factors based on Parallel Analysis (PA)
get_nfactors_pa <- function(data, indices) {
  bootstrap_data <- data[indices, ]  # Create the bootstrap sample
  pa <- psych::fa.parallel(bootstrap_data, fm = "pa", plot = FALSE)
  nfactors_pa <- pa$nfact
  return(nfactors_pa)
}

# Function to get number of factors based on Comparative Data (CD)
get_nfactors_cd <- function(data, indices) {
  bootstrap_data <- data[indices, ]  # Create the bootstrap sample
  cd <- EFAtools::CD(bootstrap_data, n_factors_max = NA, N_pop = 10000, N_samples = 500, 
                     alpha = 0.3, use = "pairwise.complete.obs", 
                     cor_method = "pearson", max_iter = 50)
  nfactors_cd <- cd$n_factors
  return(nfactors_cd)
}

# Function to get number of factors based on Minimum Average Partial (MAP) criterion
get_nfactors_map <- function(data, indices) {
  bootstrap_data <- data[indices, ]  # Create the bootstrap sample
  vss_results <- psych::VSS(bootstrap_data, rotate = "none", fm = "ml", plot = FALSE)
  nfactors_map <- which.min(vss_results$map)
  return(nfactors_map)
}

# Function to get number of factors based on Eigenvalues > 1 (and store eigenvalues)
get_nfactors_eigen <- function(data, indices) {
  bootstrap_data <- data[indices, ]  # Create the bootstrap sample
  correlation_matrix <- cor(bootstrap_data)
  eigenvalues <- eigen(correlation_matrix)$values
  nfactors_eigen <- sum(eigenvalues > 1)  # Keep eigenvalues > 1
  return(c(nfactors_eigen, eigenvalues))  # Return a vector
}

# Helper function to calculate and print 95% CI for bootstrapped fit measures
compute_ci <- function(boot_values, measure_name = NULL) {
  ci_lower <- quantile(boot_values, 0.025)
  ci_upper <- quantile(boot_values, 0.975)
  print(paste0("95% CI for ", measure_name, ": [", ci_lower, ", ", ci_upper, "]"))
  return(c(ci_lower, ci_upper))
}

knitr::opts_chunk$set(echo = TRUE)
```


```{r}

# Perform bootstrap with four different methods to help determine number of factors
boot_results_pa <- boot(data = efa_data[, questionnaire_vars], statistic = get_nfactors_pa, R = boot_iterations)
boot_results_cd <- boot(data = efa_data[, questionnaire_vars], statistic = get_nfactors_cd, R = boot_iterations)
boot_results_map <- boot(data = efa_data[, questionnaire_vars], statistic = get_nfactors_map, R = boot_iterations)
boot_results_eigen <- boot(data = efa_data[, questionnaire_vars], statistic = get_nfactors_eigen, R = boot_iterations)

knitr::opts_chunk$set(echo = TRUE)
```

## Create plots for factor extraction

```{r}

# Extract the number of factors and eigenvalues from the boot_results_eigen
boot_nfactors_eigen <- boot_results_eigen$t[, 1]  # First element is the number of factors
boot_eigenvalues <- boot_results_eigen$t[, -1]  # Remaining elements are eigenvalues

# Calculate the mean and 95% confidence interval for each eigenvalue
mean_eigenvalues <- colMeans(boot_eigenvalues, na.rm = TRUE)
ci_eigenvalues <- apply(boot_eigenvalues, 2, compute_ci)

# Create a data frame for plotting
plot_eigen <- data.frame(
  eigenvalue_index = 1:length(mean_eigenvalues),
  mean_eigenvalue = mean_eigenvalues,
  ci_lower = ci_eigenvalues[1, ],
  ci_upper = ci_eigenvalues[2, ]
)

# Create a scree plot with 95% confidence intervals and a horizontal line at eigenvalue = 1
boot_scree <- ggplot(plot_eigen, aes(x = eigenvalue_index, y = mean_eigenvalue)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +
  geom_hline(yintercept = 1, linetype="dashed", color = "red") +  # Add a horizontal line at eigenvalue = 1
  labs(
    #title = "Scree Plot with Bootstrap Eigenvalues",
    x = "Factor",
    y = "Mean Eigenvalue (95% CI)"
  ) +
  scale_x_continuous(breaks = seq(min(plot_eigen$eigenvalue_index), max(plot_eigen$eigenvalue_index), by = 1)) + # Force x-axis intervals to be integers
  theme_minimal()

scree_save_path <- file.path(path, "boot_scree.pdf")
ggsave(scree_save_path, plot = boot_scree, width = 11.69, height = 8.27)  # Save to PDF (A4)


knitr::opts_chunk$set(echo = TRUE)
```


```{r}

# Function to create histogram with mode indicated
plot_histogram <- function(nfactors, title, label) {
  df <- data.frame(nfactors = nfactors)
  mode_value <- as.numeric(names(sort(table(df$nfactors), decreasing = TRUE)[1]))  # Calculate mode
  
  ggplot(df, aes(x=nfactors)) +
    geom_histogram(binwidth=1, fill="skyblue", color="black") +
    geom_vline(aes(xintercept = mode_value), linetype="dashed", color = "red") +  # Add a vertical line at mode
    labs(title=paste(label, ": Bootstrapped Results (", title, ")"),
         x="Number of Factors",
         y="Frequency") +
    scale_x_continuous(breaks = seq(floor(min(df$nfactors)), ceiling(max(df$nfactors)), by = 1))  # Ensure x-axis only displays integers
}

# Create plots
p1 <- plot_histogram(boot_results_pa$t, "Parallel Analysis", "A")
p2 <- plot_histogram(boot_results_cd$t, "Comparative Data", "B")
p3 <- plot_histogram(boot_results_map$t, "Minimum Average Partial", "C")
p4 <- plot_histogram(boot_nfactors_eigen, "Eigenvalues > 1", "D")

grid <- arrangeGrob(p1, p2, p3, p4, ncol=2)
grid.draw(grid)

hist_save_path <- file.path(path, "nfactors_hist.pdf")
ggsave(hist_save_path, plot = grid, width = 11.69, height = 8.27)  # Save to PDF (A4)

knitr::opts_chunk$set(echo = TRUE)
```


```{r}

# Get mode (or median) of bootstrap estimates for number of factors
nfactors_final_pa <- as.integer(names(which.max(table(boot_results_pa$t))))  # or median(boot_results_pa$t)
nfactors_final_cd <- as.integer(names(which.max(table(boot_results_cd$t))))  # or median(boot_results_cd$t)
nfactors_final_map <- as.integer(names(which.max(table(boot_results_map$t))))  # or median(boot_results_map$t)
nfactors_final_eigen <- as.integer(names(which.max(table(boot_results_eigen$t))))  # or median(boot_results_eigen$t)

# Print out the results with a descriptive message
print(paste("Based on the bootstrapped PA criterion, the modal number of factors is", nfactors_final_pa))
print(paste("Based on the bootstrapped CD criterion, the modal number of factors is", nfactors_final_cd))
print(paste("Based on the bootstrapped MAP criterion, the modal number of factors that minimise the average squared partial correlation is", nfactors_final_map))
print(paste("Based on the bootstrapped Eigenvalue > 1 criterion, the modal number of factors for Eigenvalues > 1 is", nfactors_final_eigen))

# Make final decision on number of factors by asking for user input
nfactors_input <- readline(prompt="How many factors do you want to retain for the EFA? ")

knitr::opts_chunk$set(echo = TRUE)
```


### Part 2: EFA

```{r}

# Set number of factors for EFA
nfactors <- as.integer(nfactors_input)
print(paste("The number of factors to be retained for the EFA is", nfactors))

knitr::opts_chunk$set(echo = TRUE)
```

## Define functions to perform bootstrap EFA

```{r}

# Helper function to perform EFA
run_factor_analysis <- function(data, nfactors) {
  if(nfactors == 1) {
    rotate_method = 'none'
  } else {
    rotate_method = 'oblimin'
  }
  
  fa_result <- tryCatch({
    psych::fa(r = data, nfactors = nfactors, rotate = rotate_method, 
              scores = "regression", fm = "pa")
  }, error = function(e) {
    print(paste("EFA failed:", e$message))
    return(NULL)
  })
  #print(fa_result)
  return(fa_result)
}

# Function to perform Procrustes rotation of loadings from the bootstrap EFA to match loadings of the initial EFA
procrustes_rotate_loadings <- function(bootstrap_efa, initial_efa) {
  procrustes_result <- tryCatch({
    EFA.dimensions::PROCRUSTES(loadings = bootstrap_efa$loadings, target = initial_efa$loadings, type = 'oblique', verbose = FALSE)
  }, error = function(e) {
    print(paste("Procrustes rotation failed:", e$message))
    return(NULL)
  })
  if(!is.null(procrustes_result)){
    return(procrustes_result$loadingsPROC)
  } else {
    return(NULL)
  }
}

# Function to perform EFA on a bootstrap sample, rotate loadings and return them in vector form
efa_func <- function(data, indices, nfactors, initial_efa) {
  bootstrap_data <- data[indices, ]
  
  bootstrap_efa <- run_factor_analysis(bootstrap_data, nfactors)
  
  if (is.null(bootstrap_efa)) return(list(rotated = NULL, unrotated = NULL))
  
  bootstrap_efa$loadings <- bootstrap_efa$loadings[rowSums(is.na(bootstrap_efa$loadings)) != ncol(bootstrap_efa$loadings), ]
  
  unrotated_loadings_df <- as.data.frame(bootstrap_efa$loadings)
  unrotated_loadings_df$item <- colnames(data)
  
  if(nfactors == 1) {
    loadings_df <- as.data.frame(bootstrap_efa$loadings)
  } else {
    rotated_loadings <- procrustes_rotate_loadings(bootstrap_efa, initial_efa)
    if (is.null(rotated_loadings)) return(list(rotated = NULL, unrotated = unrotated_loadings_df))
    
    loadings_df <- as.data.frame(rotated_loadings)
  }
  loadings_df$item <- colnames(data)
  
  # Compute residuals
  observed_matrix <- cor(bootstrap_data)

  loadings_matrix <- bootstrap_efa$loadings
  
  estimated_matrix <- loadings_matrix %*% t(loadings_matrix) + diag(bootstrap_efa$uniquenesses)
  
  residuals_matrix <- observed_matrix - estimated_matrix
  
  return(list(rotated = loadings_df, unrotated = unrotated_loadings_df, residuals = residuals_matrix))
}

# Function to perform EFA on bootstrapped sample
run_bootstrap_efa <- function(efa_data, questionnaire_vars, boot_iterations, nfactors, initial_efa) {
  results_list_unrotated <- vector("list", boot_iterations)
  results_list_rotated <- vector("list", boot_iterations)
  residuals_list <- vector("list", boot_iterations)
  
  for (i in 1:boot_iterations) {
    result <- efa_func(efa_data[, questionnaire_vars], sample(1:nrow(efa_data), replace = TRUE), nfactors, initial_efa)
    
    # Store unrotated factor loadings
    if (!is.null(result$unrotated)) {
      results_list_unrotated[[i]] <- result$unrotated
    }
    # Store rotated factor loadings
    if (!is.null(result$rotated)) {
      results_list_rotated[[i]] <- result$rotated
    }
    # Store residuals
    if (!is.null(result$residuals)) {
      residuals_list[[i]] <- result$residuals
    }
  }
  return(list(rotated = results_list_rotated, unrotated = results_list_unrotated, residuals = residuals_list))
}

```

## Functions for summarising bootstrap EFA

```{r}

# Function returning summary of factor loadings from bootstrap EFA
loadings_summary <- function(bootstrap_results, nfactors) {
  # Create an empty list to store factor loadings for each item and factor
  factor_loadings <- vector("list", nfactors)
  names(factor_loadings) <- paste0("MR", 1:nfactors)
  
  # Loop over the list of bootstrapped factor analyses
  for(i in 1:length(bootstrap_results)) {
    for(j in 1:nfactors) {
      # If the first iteration, initialize a data.frame
      if(i == 1) {
        factor_loadings[[j]] <- data.frame(item = bootstrap_results[[i]]$item, 
                                           loading = bootstrap_results[[i]][,j],
                                           factor = names(factor_loadings)[j])
      } else {
        # If not the first iteration, bind new loadings to the existing data.frame
        factor_loadings[[j]] <- rbind(factor_loadings[[j]], 
                                      data.frame(item = bootstrap_results[[i]]$item, 
                                                 loading = bootstrap_results[[i]][,j],
                                                 factor = names(factor_loadings)[j]))
      }
    }
  }
  
  # Combine all factor loadings into one data frame
  all_loadings <- do.call(rbind, factor_loadings)
  
  # Calculate the mean and 95% confidence intervals
  results <- all_loadings %>%
    group_by(factor, item) %>%
    summarise(mean = mean(loading),
              ci_lower = quantile(loading, 0.025),
              ci_upper = quantile(loading, 0.975)) %>%
    arrange(factor, desc(mean)) %>%
    select(item, factor, mean, ci_lower, ci_upper)  # reorder columns
  
  return(results)
}

format_loadings <- function(loadings_summary) {
  
  # Transform numeric values into character with specified format
  loadings_summary <- loadings_summary %>%
    mutate(value = paste0(round(mean, 2), " (", round(ci_lower, 2), ", ", round(ci_upper, 2), ")")) %>%
    select(item, factor, value)
  
  # Use pivot_wider to spread factor values into separate columns
  formatted_loadings <- loadings_summary %>%
    pivot_wider(names_from = factor, values_from = value) %>%
    arrange(item)
  
  return(formatted_loadings)
}

# Summarise factor variances
factor_variance_summary <- function(bootstrap_results, nfactors) {
  
  # Get the total number of items
  total_items <- length(bootstrap_results[[1]][,1])
  
  # Initialize lists to store SS_loadings for each bootstrap iteration
  SS_loadings_list <- list()
  
  # Iterate over each bootstrap result to compute SS_loadings
  for (i in 1:length(bootstrap_results)) {
    SS_loadings <- sum(bootstrap_results[[i]][,1]^2)
    SS_loadings_list <- append(SS_loadings_list, SS_loadings)
  }
  
  # Convert list to vector for easier calculations
  SS_loadings_vector <- unlist(SS_loadings_list)
  
  # Calculate mean, CI for SS_loadings, and proportion of variance
  mean_SS_loadings <- mean(SS_loadings_vector)
  ci_values_SS <- compute_ci(SS_loadings_vector)
  perc_of_variance <- (mean_SS_loadings / total_items) * 100
  prop_variance_bootstrap <- mean_SS_loadings / total_items
  
  results <- data.frame(factor = paste0("MR", nfactors),
                        SS_loadings = mean_SS_loadings,
                        perc_of_variance = perc_of_variance,
                        prop_variance_bootstrap = prop_variance_bootstrap,
                        CI_lower_SS = ci_values_SS[1],
                        CI_upper_SS = ci_values_SS[2],
                        CI_lower_prop = ci_values_SS[1] / total_items,
                        CI_upper_prop = ci_values_SS[2] / total_items)
  
  return(results)
}


# Function returning summary of factor communalities from bootstrap EFA
communalities_summary <- function(loadings_list) {
  # Use do.call() and rbind to combine the list of data frames into one data frame
  communalities_df <- do.call(rbind, lapply(loadings_list, function(loadings) {
    item_names <- loadings$item # Store item names separately
    loadings <- loadings %>% select(-item) # Remove the item column from loadings data
    communalities <- rowSums(loadings^2) # Calculate communalities
    # Create a data frame containing item names and communalities
    data.frame(item = item_names, communality = communalities, uniqueness = 1 - communalities)
  }))
  
  # Summarise communalities and uniquenesses by calculating mean, CI, and adding mean communalities as a separate column
  summarized_metrics <- communalities_df %>%
    group_by(item) %>%
    summarise(
      mean_communality = round(mean(communality), 2),
      communality = paste0(round(mean(communality), 2), " (", round(quantile(communality, 0.025), 2), ", ", round(quantile(communality, 0.975), 2), ")"),
      uniqueness = paste0(round(mean(uniqueness), 2), " (", round(quantile(uniqueness, 0.025), 2), ", ", round(quantile(uniqueness, 0.975), 2), ")"),
      .groups = "drop"
    )
  return(summarized_metrics)
}

# Function returning summary of standardised residuals from bootstrap EFA
residuals_summary <- function(residuals_list) {
  # Number of items (assuming square matrices for residuals)
  p <- dim(residuals_list[[1]])[1]
  
  # Extract item labels from the first residuals matrix
  item_labels <- rownames(residuals_list[[1]])
  
  # Initialise matrices to store the diagonal and off-diagonal residuals for each bootstrap sample
  diagonal_residuals <- matrix(0, nrow=length(residuals_list), ncol=p)
  off_diagonal_residuals <- matrix(0, nrow=length(residuals_list), ncol=p)
  
  # Extract diagonal and off-diagonal residuals for each bootstrap sample
  for(i in 1:length(residuals_list)) {
    diagonal_residuals[i, ] <- diag(residuals_list[[i]])
    off_diagonal_residuals[i, ] <- rowMeans(abs(residuals_list[[i]])) - abs(diag(residuals_list[[i]]))
  }
  
  # Compute mean and 95% CIs for diagonal residuals
  mean_diag <- colMeans(diagonal_residuals)
  ci_lower_diag <- apply(diagonal_residuals, 2, function(x) quantile(x, 0.025))
  ci_upper_diag <- apply(diagonal_residuals, 2, function(x) quantile(x, 0.975))
  
  # Compute mean and 95% CIs for off-diagonal residuals
  mean_off_diag <- colMeans(off_diagonal_residuals)
  ci_lower_off_diag <- apply(off_diagonal_residuals, 2, function(x) quantile(x, 0.025))
  ci_upper_off_diag <- apply(off_diagonal_residuals, 2, function(x) quantile(x, 0.975))
  
  # Combine results into a data frame
  summary_df <- data.frame(
    item = item_labels,
    mean_diag_residual = mean_diag,
    ci_lower_diag = ci_lower_diag,
    ci_upper_diag = ci_upper_diag,
    mean_off_diag_residual = mean_off_diag,
    ci_lower_off_diag = ci_lower_off_diag,
    ci_upper_off_diag = ci_upper_off_diag
  )
  
  return(summary_df)
}

```

## Run initial EFA

```{r}

# Run initial EFA to create factor structure
initial_efa <- run_factor_analysis(efa_data[, questionnaire_vars], nfactors)

if(is.null(initial_efa)){
  print("Initial EFA failed. Check your data.")
} else {
  
  # Capture printed output of the EFA
  efa_output <- capture.output(print(initial_efa))
  efa_output_with_title <- c(paste("Initial EFA (", nfactors, "-factor model):"), efa_output)

  # Print and save captured output with title to a text file
  print(efa_output_with_title)
  writeLines(efa_output_with_title, file.path(path, "Initial_EFA.txt"))

  # Interpretation: 
  # Ideally, the initial EFA should provide insights into the factor structure. Check if specific items load strongly onto particular factors.
}
  
```
  
  
## Run bootstrap EFA
  
```{r}

  # Collect bootstrap EFA results
  bootstrap_results <- run_bootstrap_efa(efa_data, questionnaire_vars, boot_iterations, nfactors, initial_efa)
  
  # Print loadings summary
  loadings_summary_unrotated <- format_loadings(loadings_summary(bootstrap_results$unrotated, nfactors))
  
  # Save summary to a CSV file
  before_procrustes_save_path <- file.path(path, "Loadings_before_Procrustes.csv")
  write.csv(loadings_summary_unrotated, before_procrustes_save_path, row.names = FALSE)

  
  # Interpretation: 
  # Examine the loadings to see which items consistently load onto specific factors. Items with loadings close to 0 may not fit well with any factor.
  
  loadings_summary_rotated <- loadings_summary(bootstrap_results$rotated, nfactors)
  formatted_loadings_summary_rotated <- format_loadings(loadings_summary_rotated)

  # Print summary
  print("Final loadings summary after Procrustes rotations:")
  print(formatted_loadings_summary_rotated)
  
  # Save summary to a CSV file
  after_procrustes_save_path <- file.path(path, "Loadings_after_Procrustes.csv")
  write.csv(formatted_loadings_summary_rotated, after_procrustes_save_path, row.names = FALSE)


  # Interpretation:
  # Procrustes rotations help align the loadings of bootstrap EFAs with the initial EFA. Check if the rotated loadings offer clearer insights into the factor structure.
  
  variance_results <- factor_variance_summary(bootstrap_results$rotated, nfactors)

  # Print summary
  print("Explained Variance by Factors")
  print(variance_results)
  
  # Save summary to a CSV file
  variance_save_path <- file.path(path, "Explained_variance.csv")
  write.csv(variance_results, variance_save_path, row.names = FALSE)


  # Interpretation:
  # Factors that explain a higher proportion of the variance are generally more significant. Factors explaining very little variance may be less informative or redundant.
  
  communalities_summary_rotated <- communalities_summary(bootstrap_results$rotated)
  
  # Print communalities summary
  print("Communalities summary after Procrustes rotations:")
  print(communalities_summary_rotated)
  
  # Save summary to a CSV file
  variance_save_path <- file.path(path, "Communalities.csv")
  write.csv(communalities_summary_rotated, variance_save_path, row.names = FALSE)

  
  # Interpretation:
  # Communalities reflect how well each item is represented by the factors. Items with low communalities might not fit well with the chosen factor structure.

  summarized_residuals <- residuals_summary(bootstrap_results$residuals)

  # Print residuals summary
  print("Summary of standardised residuals:")
  print(summarized_residuals)
  
  # Save summary to a CSV file
  residuals_save_path <- file.path(path, "Residuals.csv")
  write.csv(summarized_residuals, residuals_save_path, row.names = FALSE)

```


```{r}

# Convert the loading matrix to long format for heatmap
heatmap_data <- loadings_summary_rotated %>%
  pivot_wider(names_from = factor, values_from = mean, id_cols = item)

# Create a function to generate the heatmap for a given factor
plot_heatmap <- function(factor_name) {
  ggplot(heatmap_data, aes(x = factor, y = item)) + 
    geom_tile(aes_string(fill = factor_name), color = "white") +  # Use aes_string to dynamically use the column name
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
    theme_minimal() +
    labs(title = paste("Heatmap of Factor Loadings -", factor_name), 
         x = "Factors", y = "Items", fill = "Loading")
}

# Create a list of factor names
factor_names <- paste0("MR", 1:nfactors)

factor_names %>%
  purrr::map(plot_heatmap) %>%
  gridExtra::grid.arrange(grobs = ., ncol = nfactors)

# Bar Chart for Variance Explained
ggplot(variance_results, aes(x=factor, y=perc_of_variance)) +
  geom_bar(stat="identity", fill="steelblue") +
  theme_minimal() +
  labs(title="Variance Explained by Each Factor", x="Factor", y="Variance Explained (%)")

# Obtain summarized metrics with mean communalities
communalities_summary_rotated_modified <- communalities_summary(bootstrap_results$rotated)

# Histogram of Communalities
ggplot(communalities_summary_rotated_modified, aes(x=mean_communality)) +
  geom_histogram(fill="skyblue", color="black", binwidth=0.05) +
  theme_minimal() +
  labs(title="Histogram of Mean Communalities", x="Communality", y="Frequency")

knitr::opts_chunk$set(echo = TRUE)
```


## Part 3: Remove any unwanted items

```{r}

# Set cutoff for factor loadings (weak item loading and factor cross-loading)
loadings_threshold <- 0.32

# Identify the primary factor for each item
loadings_summary_rotated <- loadings_summary_rotated %>%
  group_by(item) %>%
  mutate(primary_factor = factor[which.max(mean)]) %>%
  ungroup()

# Function to identify items with weak loadings (using mean)
find_weak_loading_mean <- function(loadings_summary, loadings_threshold) {
  # Group by item and identify the primary factor for each item
  # Check if the primary factor's mean loading is below the threshold
  weak_loading_items <- loadings_summary %>%
    group_by(item) %>%
    mutate(primary_factor = factor[which.max(mean)],
           max_mean_primary = max(mean[factor == primary_factor])) %>%
    ungroup() %>%
    # Filter items where the primary factor's mean loading is below the threshold
    filter(max_mean_primary <= loadings_threshold) %>%
    # Extract unique items
    distinct(item) %>%
    pull(item)
  
  # Return a list of items with weak loadings
  return(weak_loading_items)
}

# Function to identify items with weak loadings (using lower 95% CI)
find_weak_loading_ci <- function(loadings_summary, loadings_threshold) {
  # Group by item and identify the primary factor for each item
  # Compute the lower bound of the confidence interval for the primary factor
  weak_loading_items <- loadings_summary %>%
    group_by(item) %>%
    mutate(primary_factor = factor[which.max(mean)],
           max_ci_lower_primary = max(ci_lower[factor == primary_factor])) %>%
    ungroup() %>%
    # Filter items where the primary factor's CI lower bound is below the threshold
    filter(max_ci_lower_primary <= loadings_threshold) %>%
    # Extract unique items
    distinct(item) %>%
    pull(item)
  
  # Return a list of items with weak loadings
  return(weak_loading_items)
}

# Function to identify items with cross-loadings (using the mean)
find_cross_loading_mean <- function(loadings_summary, loadings_threshold) {
  # Group by item and count the number of factors where the mean loading is above the threshold
  cross_loading_items <- loadings_summary %>%
    group_by(item) %>%
    mutate(cross_loading_count = sum(mean >= loadings_threshold)) %>%
    ungroup() %>%
    # Filter items that load onto more than one factor based on the mean loading threshold
    filter(cross_loading_count > 1) %>%
    # Extract unique items
    distinct(item) %>%
    pull(item)
  
  # Return a list of items with cross-loadings
  return(cross_loading_items)
}

# Function to identify items with cross-loadings (using lower 95% CI)
find_cross_loading_ci <- function(loadings_summary, loadings_threshold) {
  # Group by item and count the number of factors where the CI lower bound is above the threshold
  cross_loading_items <- loadings_summary %>%
    group_by(item) %>%
    mutate(cross_loading_count = sum(ci_lower >= loadings_threshold)) %>%
    ungroup() %>%
    # Filter items that load onto more than one factor based on the CI lower bound threshold
    filter(cross_loading_count > 1) %>%
    # Extract unique items
    distinct(item) %>%
    pull(item)
  
  # Return a list of items with cross-loadings
  return(cross_loading_items)
}

knitr::opts_chunk$set(echo = TRUE)
```


```{r}

# Identify items to remove
weak_loading_items <- find_weak_loading_ci(loadings_summary_rotated, loadings_threshold)
cross_loading_items <- find_cross_loading_ci(loadings_summary_rotated, loadings_threshold)

# Print items meeting each criterion
if(length(weak_loading_items) > 0) {
  print(paste("The items that meet the criterion for weak loading", loadings_threshold, ") are:", paste(weak_loading_items, collapse = ", ")))
} else {
  print(paste("No items met the criterion for weak loading.", loadings_threshold, ")."))
}

if(length(cross_loading_items) > 0) {
  print(paste("The items that meet the criterion for cross-loading (i.e., 95% CIs included", loadings_threshold, "in more than one factor) are:", paste(cross_loading_items, collapse = ", ")))
} else {
  print(paste("No items met the criterion for cross-loading (i.e., 95% CIs did not include", loadings_threshold, "in more than one factor)."))
}

# Items that meet more than one criteria
multiple_criteria_items <- intersect(weak_loading_items, cross_loading_items)
if(length(multiple_criteria_items) > 0) {
  print(paste("The items removed due to meeting more than one criteria are:", paste(multiple_criteria_items, collapse = ", ")))
} else {
  print("No items met more than one criteria.")
}

knitr::opts_chunk$set(echo = TRUE)
```


```{r}

# Create final list of items to remove
items_to_remove <- union(weak_loading_items, cross_loading_items)

# Remove any items from CFA data that fit removal criteria and get remaining questionnaire variables
if(length(items_to_remove) > 0) {
  # Remove these items from the CFA dataset
  cfa_data_rmv <- cfa_data %>%
    select(-one_of(items_to_remove))
  
  print(paste("The following items were removed from the CFA dataset:", paste(items_to_remove, collapse = ", ")))
  
  # Get remaining questionnaire variables (all retained variables except 'record_id')
  cfa_questionnaire_vars <- names(cfa_data_rmv)[!(names(cfa_data_rmv) %in% "record_id")]
  print("The following items will be used in the CFA:")
  print(cfa_questionnaire_vars)
  
} else {
  cfa_data_rmv <- cfa_data
  cfa_questionnaire_vars <- questionnaire_vars
  print("No items were removed from the CFA dataset.")
}

knitr::opts_chunk$set(echo = TRUE)
```


### Part 4: Bootstrapped CFA

```{r}

# Create an empty vector to store the CFA model syntax
initial_cfa_model_syntax <- ""

# Group items by their primary factors
grouped_items <- loadings_summary_rotated %>%
  filter(mean > loadings_threshold, item %in% cfa_questionnaire_vars) %>%  # Filter on cfa_questionnaire_vars
  group_by(primary_factor) %>%
  summarise(items = paste(item, collapse = " + "),
            .groups = "drop")

# Generate the CFA model syntax
for(i in 1:nrow(grouped_items)) {
  factor <- grouped_items$primary_factor[i]
  items <- grouped_items$items[i]
  initial_cfa_model_syntax <- paste(initial_cfa_model_syntax, factor, "=~", items, "\n")
}

# Print the CFA model syntax
print(initial_cfa_model_syntax)

knitr::opts_chunk$set(echo = TRUE)
```

### Run Bootstrap CFA

```{r}

# Fit initial CFA model using lavaan
initial_cfa_model <- lavaan::cfa(initial_cfa_model_syntax, data = cfa_data_rmv, missing = "fiml")

# Create summary
initial_cfa <- summary(initial_cfa_model, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# Capture printed summary of initial CFA model
initial_cfa_output <- capture.output(print(initial_cfa))

# Custom function to extract measures from bootstrapLavaan
custom_extract <- function(lavaan_obj) {
  
  # Extract fit measures
  fit_measures <- lavaan::fitMeasures(lavaan_obj, fit.measures = c("chisq", "cfi", "tli", "rmsea", "srmr", "aic", "bic"))
  
  # Add prefix to fit measures
  names(fit_measures) <- paste0("fit_", names(fit_measures))
   
  # Extract parameter estimates
  param_estimates <- lavaan::parameterEstimates(lavaan_obj)
  
  # Create parameter names with prefix and structure
  param_names <- paste("param_", param_estimates$lhs, param_estimates$op, param_estimates$rhs, sep="")
  params <- setNames(param_estimates$est, param_names)
  
  # Extract coefficients
  coefs <- lavaan::coef(lavaan_obj)
  names(coefs) <- paste0("coef_", names(coefs))
  
  # Combine the fit measures, coefficients, and parameter estimates
  combined_results <- c(fit_measures, coefs, params)
  
  # Return the combined results
  return(combined_results)
}


# Bootstrap the initial CFA model
boot_cfa_model <- lavaan::bootstrapLavaan(initial_cfa_model, 
                                          R = boot_iterations, 
                                          iseed = seed, 
                                          FUN = custom_extract
                                          )

```

### Extract bootstrap CFA results

```{r}

# Get summary of the bootstrapped CFA model
boot_cfa_summary <- summary(boot_cfa_model, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)

# Capture printed output of the bootstrapped CFA
boot_cfa_output <- capture.output(print(boot_cfa_summary))

# Extract all bootstrapped measures and coefficients from boot_cfa_model
boot_measures <- list()
for (name in colnames(boot_cfa_model)) {
  boot_measures[[name]] <- boot_cfa_model[, name]
}

# Calculate the 2.5th and 97.5th percentiles for the 95% confidence interval for each measure
ci_measures <- lapply(boot_measures, compute_ci)

# Get the test statistic and degrees of freedom for the original sample
T_orig <- fitMeasures(initial_cfa_model, "chisq")
df_orig <- fitMeasures(initial_cfa_model, "df")

# Compute a bootstrap based p-value for chi-square
pvalue_boot <- length(which(boot_measures$fit_chis > T_orig)) / length(boot_measures$fit_chis)

# Capture the final output
final_output <- capture.output({
  cat("Initial CFA model:\n")
  cat(initial_cfa_output, sep="\n")
  cat("\nBootstrapped CFA model:", nfactors, "-factor model\n")
  cat(boot_cfa_output, sep="\n")
  cat("\nConfidence Intervals for Bootstrapped Measures:\n")
  for (name in names(ci_measures)) {
    cat(name, ":", ci_measures[[name]], "\n")
  }
  cat("\nTest Statistic for the Original Sample (Chi-Square):", T_orig, "\n")
  cat("Degrees of Freedom for chi-square test:", df_orig, "\n")
  cat("Bootstrap chi-square p-value:", pvalue_boot, "\n")
})

# Save captured output to a text file
writeLines(final_output, file.path(path, "CFA_Summary.txt"))

```


### Create path diagram for initial CFA

```{r}

# Plot the initial CFA model
initial_cfa_plot <- semPaths(initial_cfa_model, 
                             whatLabels = "est", 
                             edge.label.cex = 0.7, 
                             layout = "tree", 
                             intercepts = FALSE, 
                             residuals = FALSE, 
                             style = "lisrel", 
                             curveAdjacent = TRUE, 
                             rotation = 2)

# Display the plot
initial_cfa_plot

```

### Compare parameters from initial and bootstrap CFA

```{r}

# Extract parameter names from the initial CFA
initial_param_estimates <- parameterEstimates(initial_cfa_model)

# Filter out only the factor loadings from the parameter estimates
initial_factor_loadings <- initial_param_estimates$est[initial_param_estimates$op == "=~"]

# Print out the initial factor loadings
print("Initial CFA Factor Loadings:")
print(initial_factor_loadings)

initial_cfa_params <- paste(initial_param_estimates$lhs, initial_param_estimates$op, initial_param_estimates$rhs, sep="")
print(initial_cfa_params)

# Select parameters starting with "param_" from boot_cfa_params
boot_param_keys <- colnames(boot_cfa_model)[grepl("^param_", colnames(boot_cfa_model))]

# Remove the "param_" prefix from these keys
boot_param_keys <- gsub("^param_", "", boot_param_keys)

# Check existence of each parameter from the initial CFA model in boot_param_means
params_exist_in_boot <- initial_cfa_params %in% boot_param_keys
print(params_exist_in_boot)

# Find parameters in initial CFA that are not in bootstrap CFA
missing_in_boot <- initial_cfa_params[!params_exist_in_boot]

# Check for parameters in the initial CFA model but not in boot_param_means
if (length(missing_in_boot) > 0) {
    print(paste("Parameters in initial CFA model but not in boot_param_means:", paste(missing_in_boot, collapse = ", ")))
} else {
    print("All parameters from the initial CFA model are present in boot_param_means.")
}

# Find parameters in bootstrap CFA that are not in initial CFA
missing_in_initial <- boot_param_keys[!boot_param_keys %in% initial_cfa_params]

# Check for parameters in boot_param_means but not in the initial CFA model
if (length(missing_in_initial) > 0) {
    print(paste("Parameters in boot_param_means but not in initial CFA model:", paste(missing_in_initial, collapse = ", ")))
} else {
    print("All parameters from boot_param_means are present in the initial CFA model.")
}

# Compute the mean for each bootstrapped 'param_' parameter
boot_param_means <- sapply(boot_param_keys, function(param) {
    mean(boot_cfa_model[, paste0("param_", param)])
})

# Print out the means
print("Bootstrapped Parameter Means:")
print(boot_param_means)

```

### Create path diagram for bootstrap CFA

```{r}

# Create a new plot object for the updated plot with bootstrapped means
boot_cfa_plot <- initial_cfa_plot

# Filter boot_param_means to only include relationships (factor loadings)
boot_factor_loadings <- boot_param_means[grep("=~", names(boot_param_means))]

# List to store formatted labels
formatted_labels <- vector("character", length(boot_factor_loadings))

# Iterate over each factor loading to format labels
for (i in 1:length(boot_factor_loadings)) {
  # Extract the parameter name
  param_name <- names(boot_factor_loadings)[i]
  
  # Extract bootstrapped values for the factor loading from boot_cfa_model
  boot_values <- boot_cfa_model[, paste0("param_", param_name)]
  
  # Calculate 95% CI
  ci <- compute_ci(boot_values, param_name)
  
  # Format the label
  formatted_labels[i] <- paste0(round(boot_factor_loadings[i], 2), " (", round(ci[1], 2), ", ", round(ci[2], 2), ")")
}

# Update boot_cfa_plot with the new formatted labels
boot_cfa_plot$Arguments$edge.labels <- formatted_labels

# Truncate the item names to only the first word using parameter names
original_item_names <- sapply(strsplit(names(boot_factor_loadings), "~"), `[`, 2)
truncated_labels <- sapply(strsplit(original_item_names, "_"), `[`, 1)
boot_cfa_plot$Arguments$labels[1:length(truncated_labels)] <- truncated_labels

# Calculate the width based on the number of characters in the longest label
max_width <- max(nchar(truncated_labels))

# Set max character limit for item labels
max_char <- 10 # Adjust this as per requirements

# Set a maximum width if needed
max_allowed_width <- max_char
if (max_width > max_allowed_width) {
  max_width <- max_allowed_width
}

# Adjust the width of only the observed variable nodes in the plot to this calculated width
boot_cfa_plot$Arguments$vsize[1:length(truncated_labels)] <- rep(max_width, length(truncated_labels))

# Extracting edge labels from initial_cfa_plot
initial_labels <- initial_cfa_plot$Arguments$edge.labels

# Extracting edge labels from boot_cfa_plot
boot_labels <- boot_cfa_plot$Arguments$edge.labels

# Printing out the values for comparison
print("Initial CFA Plot Edge Labels:")
print(initial_labels)

print("Bootstrapped CFA Plot Edge Labels with 95% CI:")
print(boot_labels)

# Display the updated plot
qgraph::qgraph(boot_cfa_plot)


```
